{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxAZptR7phpk"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Set a maximum number of employees\n",
    "MAX_EMPLOYEES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrHKDHaRqzc-",
    "outputId": "5bb127d6-b4a9-448f-95b9-e835ec21915f"
   },
   "outputs": [],
   "source": [
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths to your datasets\n",
    "task_data_path = '/content/drive/MyDrive/Skripsi/Resources/task_test.csv'\n",
    "employee_data_path = '/content/drive/MyDrive/Skripsi/Resources/employee_test.csv'\n",
    "\n",
    "# Load the datasets\n",
    "df_tasks_original = pd.read_csv(task_data_path)\n",
    "df_employees_original = pd.read_csv(employee_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure employees count is always MAX_EMPLOYEES\n",
    "if len(df_employees_original) < MAX_EMPLOYEES:\n",
    "    # Pad with dummy employees\n",
    "    num_to_add = MAX_EMPLOYEES - len(df_employees_original)\n",
    "    new_rows = pd.DataFrame([{\n",
    "        'employee_id': f'Dummy {i+1}',\n",
    "        'Role': 'Dummy',\n",
    "        'Mathematics.Linear Algebra': 1,\n",
    "        'Mathematics.Differential Equations': 1,\n",
    "        'Mathematics.Optimization Technique': 1\n",
    "    } for i in range(num_to_add)])\n",
    "    df_employees_original = pd.concat([df_employees_original, new_rows], ignore_index=True)\n",
    "elif len(df_employees_original) > MAX_EMPLOYEES:\n",
    "    # Truncate\n",
    "    df_employees_original = df_employees_original.iloc[:MAX_EMPLOYEES]\n",
    "\n",
    "df_employees = df_employees_original.reset_index(drop=True)\n",
    "df_tasks = df_tasks_original.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_euclidean_distance(employee_skills, task_skills, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the normalized Weighted Euclidean Distance (WED) between an employee and a task.\n",
    "    \n",
    "    Args:\n",
    "        employee_skills (array-like): Skill levels of the employee.\n",
    "        task_skills (array-like): Skill requirements of the task.\n",
    "        alpha (float): Weighting parameter to penalize overqualification.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized Weighted Euclidean Distance (0 to 1).\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays\n",
    "    employee_skills = np.array(employee_skills)\n",
    "    task_skills = np.array(task_skills)\n",
    "\n",
    "    # Compute the current WED\n",
    "    weights = 1 / (1 + alpha * np.maximum(0, employee_skills - task_skills))\n",
    "    wed = np.sqrt(np.sum(weights * (employee_skills - task_skills) ** 2))\n",
    "\n",
    "    # Compute the worst-case scenario (max WED)\n",
    "    max_task_skills = np.full(len(task_skills), 5)  # Assuming max skill requirement is 5\n",
    "    min_employee_skills = np.zeros(len(employee_skills))  # Assuming min skill level is 0\n",
    "\n",
    "    # Calculate the actual WED\n",
    "    max_weights = 1 / (1 + alpha * np.maximum(0, min_employee_skills - max_task_skills))\n",
    "    max_wed = np.sqrt(np.sum(max_weights * (min_employee_skills - max_task_skills) ** 2))\n",
    "\n",
    "    # Normalize the WED (1 is best match, 0 is worst match)\n",
    "    normalized_wed = 1 - (wed / max_wed)\n",
    "\n",
    "    return normalized_wed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMgdaO1spjjj"
   },
   "outputs": [],
   "source": [
    "# Task Assignment Environment\n",
    "class TaskAssignmentEnv:\n",
    "    def __init__(self, tasks, employees, max_workload=20, alpha=0.5):\n",
    "        self.tasks = tasks\n",
    "        self.employees = employees\n",
    "        self.max_workload = max_workload\n",
    "        self.alpha = alpha\n",
    "        self.num_tasks = len(tasks)\n",
    "        self.num_employees = len(employees)\n",
    "\n",
    "        self.state = np.zeros((self.num_tasks, self.num_employees))\n",
    "        self.workloads = np.zeros(self.num_employees)\n",
    "\n",
    "    def valid_actions(self, task_idx):\n",
    "        valid = []\n",
    "        for i in range(self.num_employees):\n",
    "            if 'Dummy' in self.employees.iloc[i]['Role']:\n",
    "                continue\n",
    "            if self.workloads[i] + self.tasks.iloc[task_idx]['story_points'] <= self.max_workload:\n",
    "                valid.append(i)\n",
    "        return valid\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((self.num_tasks, self.num_employees))\n",
    "        self.workloads = np.zeros(self.num_employees)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, task_idx, employee_idx):\n",
    "        task = self.tasks.iloc[task_idx]\n",
    "        employee = self.employees.iloc[employee_idx]\n",
    "        task_skills = task[3:].values\n",
    "        employee_skills = employee[2:].values\n",
    "\n",
    "        # Use global WED function\n",
    "        similarity_score = calculate_weighted_euclidean_distance(employee_skills, task_skills, self.alpha)\n",
    "\n",
    "        # Update state\n",
    "        self.state[task_idx, employee_idx] = 1\n",
    "        self.workloads[employee_idx] += task['story_points']\n",
    "\n",
    "        # Calculate workload balance score\n",
    "        std_workload = np.std(self.workloads)\n",
    "        workload_balance_score = 1 / (1 + std_workload)\n",
    "\n",
    "        # Reward calculation\n",
    "        reward = 0.8 * similarity_score + 0.2 * workload_balance_score\n",
    "\n",
    "        # Done condition\n",
    "        done = np.all(self.state.sum(axis=1)) or np.any(self.workloads > self.max_workload)\n",
    "\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmidLweEplG6"
   },
   "outputs": [],
   "source": [
    "# PPO Agent Class\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def get_action(self, state, valid_actions):\n",
    "        logits, _ = self.forward(state)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        if not valid_actions:\n",
    "            return 0, torch.tensor(0.0)\n",
    "        probs = probs[valid_actions]\n",
    "        action_dist = Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return valid_actions[action.item()], action_dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjrMFdYlpmrS"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, agent, lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=4):\n",
    "        self.agent = agent\n",
    "        self.optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def compute_returns(self, rewards, dones, next_value):\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train(self, states, actions, log_probs, rewards, dones, next_value):\n",
    "        returns = self.compute_returns(rewards, dones, next_value)\n",
    "        returns = torch.tensor(returns).float()\n",
    "        states = torch.tensor(states).float()\n",
    "        actions = torch.tensor(actions).long()\n",
    "        log_probs = torch.tensor(log_probs).float()\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            logits, state_values = self.agent(states)\n",
    "            action_dist = Categorical(torch.softmax(logits, dim=-1))\n",
    "            new_log_probs = action_dist.log_prob(actions)\n",
    "\n",
    "            # Policy Loss\n",
    "            ratios = torch.exp(new_log_probs - log_probs)\n",
    "            advantages = returns - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Value Loss\n",
    "            value_loss = self.mse_loss(state_values, returns)\n",
    "\n",
    "            # Total Loss\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utR4HNslpn_b",
    "outputId": "3026d723-84e0-4eb1-edc5-0da439b56d60"
   },
   "outputs": [],
   "source": [
    "# Create environment with fixed employees\n",
    "env = TaskAssignmentEnv(df_tasks, df_employees)\n",
    "state_dim = env.state.size\n",
    "action_dim = MAX_EMPLOYEES\n",
    "\n",
    "agent = PPOAgent(state_dim, action_dim)\n",
    "ppo = PPO(agent)\n",
    "\n",
    "num_episodes = 1000\n",
    "task_assignments = {}\n",
    "convergence_threshold = 1e-2\n",
    "convergence_patience = 100\n",
    "reward_history = []\n",
    "similarity_scores = []\n",
    "workload_distributions = []\n",
    "std_workloads = []\n",
    "final_similarity_scores = []  # To store final similarity scores\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    task_idx = 0\n",
    "    episode_similarity_scores = []\n",
    "    episode_workload = np.zeros(env.num_employees)\n",
    "    task_assignments_episode = {}  # Store assignments for this episode if needed\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state.flatten(), dtype=torch.float32)\n",
    "        valid_actions = env.valid_actions(task_idx=task_idx)\n",
    "        action, log_prob = agent.get_action(state_tensor, valid_actions)\n",
    "        next_state, reward, done = env.step(task_idx=task_idx, employee_idx=action)\n",
    "\n",
    "        # Store task assignment for this episode if you need it\n",
    "        task_assignments_episode[env.tasks.iloc[task_idx]['task_id']] = env.employees.iloc[action]['employee_id']\n",
    "\n",
    "        # Similarity and workload tracking...\n",
    "        task = env.tasks.iloc[task_idx]\n",
    "        employee = env.employees.iloc[action]\n",
    "        task_skills = task[3:].values\n",
    "        employee_skills = employee[2:].values\n",
    "\n",
    "        # Use global WED function\n",
    "        distance = calculate_weighted_euclidean_distance(employee_skills, task_skills, env.alpha)\n",
    "\n",
    "        # Append the numbers\n",
    "        episode_similarity_scores.append(distance)\n",
    "        episode_workload[action] += task['story_points']\n",
    "\n",
    "        # Train PPO\n",
    "        ppo.train(state_tensor.unsqueeze(0), [action], [log_prob], [reward], [done], next_value=0)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if np.any(env.state[task_idx]):\n",
    "            task_idx += 1\n",
    "        if task_idx >= env.num_tasks:\n",
    "            done = True\n",
    "\n",
    "    similarity_scores.append(episode_similarity_scores)\n",
    "    workload_distributions.append(episode_workload)\n",
    "    std_workloads.append(np.std(episode_workload))\n",
    "    reward_history.append(episode_reward)\n",
    "    final_similarity_scores = episode_similarity_scores\n",
    "\n",
    "    # Only print minimal info here\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    # Check for convergence\n",
    "    if len(reward_history) > convergence_patience:\n",
    "        recent_rewards = reward_history[-convergence_patience:]\n",
    "        if max(recent_rewards) - min(recent_rewards) < convergence_threshold:\n",
    "            print(f\"Convergence achieved at episode {episode}. Stopping training.\")\n",
    "            break\n",
    "\n",
    "# After training completes, print final task assignment strategy if needed\n",
    "print(\"\\nFinal Task Assignment Strategy:\")\n",
    "for t_id, e_id in task_assignments_episode.items():\n",
    "    print(f\"Task {t_id} assigned to Employee {e_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qOP4Og3VsTZ_",
    "outputId": "b79cb813-7667-4c89-bfc9-d6ca885b09cf"
   },
   "outputs": [],
   "source": [
    "# Visualize final similarity scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(final_similarity_scores)\n",
    "plt.title('Distribution of Final Weighted Euclidean Distance (Similarity Scores)')\n",
    "plt.ylabel('Similarity Score')\n",
    "plt.xlabel('Final Assignments')\n",
    "plt.show()\n",
    "\n",
    "# Get the final workload distribution (only for real employees)\n",
    "real_employee_count = len(df_employees_original)  # Only include the actual number of employees\n",
    "final_workloads = workload_distributions[-1][:real_employee_count]  # Filter for real employees only\n",
    "\n",
    "# Plot the final workload distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    range(len(final_workloads)),\n",
    "    final_workloads,\n",
    "    tick_label=[f'Employee {i+1}' for i in range(len(final_workloads))]\n",
    ")\n",
    "plt.title('Final Workload Distribution')\n",
    "plt.xlabel('Employee')\n",
    "plt.ylabel('Total Workload (Story Points)')\n",
    "plt.show()\n",
    "\n",
    "# Plot the std deviation progress \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(std_workloads, label='Std Dev of Workloads')\n",
    "plt.title('Standard Deviation of Workloads Across Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Standard Deviation of Workloads')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldv5Csqco7lB"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(agent.state_dict(), '/content/drive/MyDrive/Skripsi/Resources/trained_ppo_agent.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzBPUaGqo8t_",
    "outputId": "cf7f5bc5-5a0d-4e42-c590-8665a86f7c80"
   },
   "outputs": [],
   "source": [
    "# Testing on new dataset with same approach\n",
    "new_task_data_path = '/content/drive/MyDrive/Skripsi/Resources/task_test2.csv'\n",
    "new_employee_data_path = '/content/drive/MyDrive/Skripsi/Resources/employee_test2.csv'\n",
    "\n",
    "new_df_tasks_original = pd.read_csv(new_task_data_path)\n",
    "new_df_employees_original = pd.read_csv(new_employee_data_path)\n",
    "\n",
    "# Adjust employees again\n",
    "if len(new_df_employees_original) < MAX_EMPLOYEES:\n",
    "    num_to_add = MAX_EMPLOYEES - len(new_df_employees_original)\n",
    "    new_rows = pd.DataFrame([{\n",
    "        'employee_id': f'Dummy {i+1}',\n",
    "        'Role': 'Dummy',\n",
    "        'Mathematics.Linear Algebra': 1,\n",
    "        'Mathematics.Differential Equations': 1,\n",
    "        'Mathematics.Optimization Technique': 1\n",
    "    } for i in range(num_to_add)])\n",
    "    new_df_employees_original = pd.concat([new_df_employees_original, new_rows], ignore_index=True)\n",
    "elif len(new_df_employees_original) > MAX_EMPLOYEES:\n",
    "    new_df_employees_original = new_df_employees_original.iloc[:MAX_EMPLOYEES]\n",
    "\n",
    "new_df_employees = new_df_employees_original.reset_index(drop=True)\n",
    "new_df_tasks = new_df_tasks_original.reset_index(drop=True)\n",
    "\n",
    "new_env = TaskAssignmentEnv(new_df_tasks, new_df_employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXo5l4O3o-f9"
   },
   "outputs": [],
   "source": [
    "# Recreate agent for new environment state dimension (should match)\n",
    "new_state_dim = new_env.state.size\n",
    "new_agent = PPOAgent(new_state_dim, MAX_EMPLOYEES)\n",
    "pretrained_dict = torch.load('/content/drive/MyDrive/Skripsi/Resources/trained_ppo_agent.pth')\n",
    "model_dict = new_agent.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "model_dict.update(pretrained_dict)\n",
    "new_agent.load_state_dict(model_dict)\n",
    "new_agent.eval()\n",
    "\n",
    "task_assignments = {}\n",
    "state = new_env.reset()\n",
    "done = False\n",
    "task_idx = 0\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.tensor(state.flatten(), dtype=torch.float32)\n",
    "    valid_actions = new_env.valid_actions(task_idx=task_idx)\n",
    "    action, _ = new_agent.get_action(state_tensor, valid_actions)\n",
    "    next_state, _, done = new_env.step(task_idx=task_idx, employee_idx=action)\n",
    "    task_assignments[new_df_tasks.iloc[task_idx]['task_id']] = new_df_employees.iloc[action]['employee_id']\n",
    "    state = next_state\n",
    "    if np.any(new_env.state[task_idx]):\n",
    "        task_idx += 1\n",
    "    if task_idx >= new_env.num_tasks:\n",
    "        done = True\n",
    "\n",
    "print(\"\\nTask Assignment Strategy for New Dataset:\")\n",
    "for task_id, employee_id in task_assignments.items():\n",
    "    print(f\"Task {task_id} assigned to Employee {employee_id}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
