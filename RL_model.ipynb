{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxAZptR7phpk"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrHKDHaRqzc-",
    "outputId": "5bb127d6-b4a9-448f-95b9-e835ec21915f"
   },
   "outputs": [],
   "source": [
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths to your datasets\n",
    "task_data_path = '/content/drive/MyDrive/Skripsi/Resources/Datasets/data_task_categorized.csv' \n",
    "employee_data_path = '/content/drive/MyDrive/Skripsi/Resources/Datasets/data_employee_categorized.csv'\n",
    "\n",
    "# Load the datasets\n",
    "df_tasks_original = pd.read_csv(task_data_path)\n",
    "df_employees_original = pd.read_csv(employee_data_path)\n",
    "\n",
    "# Extract skill columns from tasks and employees as NumPy arrays\n",
    "# Keeping indexing: tasks start at column 3, employees at column 2\n",
    "task_skill_cols = set(df_tasks_original.columns[3:])  # Assuming skills start at column 3\n",
    "employee_skill_cols = set(df_employees_original.columns[2:])  # Assuming skills start at column 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both tasks and employees share the same set of skill columns.\n",
    "# If there's a mismatch, take the intersection of the skill columns.\n",
    "common_skills = task_skill_cols.intersection(employee_skill_cols)\n",
    "\n",
    "# Filter both dataframes to these common skill columns plus their ID columns.\n",
    "df_tasks_original = df_tasks_original[['task_id', 'project_id', 'story_points'] + list(common_skills)]\n",
    "df_employees_original = df_employees_original[['employee_id', 'Role'] + list(common_skills)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curriculum Data Loader\n",
    "def load_curriculum_data(task_data, employee_data, iteration, task_factor=5, employee_factor=2):\n",
    "    \"\"\"\n",
    "    Dynamically load a subset of tasks and employees based on the iteration.\n",
    "    iteration n:\n",
    "      - num_tasks = n * task_factor\n",
    "      - num_employees = n * employee_factor\n",
    "    \"\"\"\n",
    "    num_tasks = iteration * task_factor\n",
    "    num_employees = iteration * employee_factor\n",
    "\n",
    "    selected_tasks = task_data.head(num_tasks).reset_index(drop=True)\n",
    "    selected_employees = employee_data.head(num_employees).reset_index(drop=True)\n",
    "\n",
    "    return selected_tasks, selected_employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_euclidean_distance(employee_skills, task_skills, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the normalized Weighted Euclidean Distance (WED) between an employee and a task.\n",
    "    \n",
    "    Args:\n",
    "        employee_skills (array-like): Skill levels of the employee.\n",
    "        task_skills (array-like): Skill requirements of the task.\n",
    "        alpha (float): Weighting parameter to penalize overqualification.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized Weighted Euclidean Distance (0 to 1).\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays\n",
    "    employee_skills = np.array(employee_skills)\n",
    "    task_skills = np.array(task_skills)\n",
    "\n",
    "    # Compute the WED\n",
    "    weights = 1 / (1 + alpha * np.maximum(0, employee_skills - task_skills))\n",
    "    wed = np.sqrt(np.sum(weights * (employee_skills - task_skills) ** 2))\n",
    "\n",
    "    # Compute worst-case WED\n",
    "    max_task_skills = np.full(len(task_skills), 5)  # Max skill requirement\n",
    "    min_employee_skills = np.zeros(len(employee_skills))  # Min skill level is 0\n",
    "\n",
    "    # Calculate the actual WED  \n",
    "    max_weights = 1 / (1 + alpha * np.maximum(0, min_employee_skills - max_task_skills))\n",
    "    max_wed = np.sqrt(np.sum(max_weights * (min_employee_skills - max_task_skills) ** 2))\n",
    "\n",
    "    # Normalize\n",
    "    normalized_wed = 1 - (wed / max_wed)\n",
    "    return normalized_wed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMgdaO1spjjj"
   },
   "outputs": [],
   "source": [
    "# Task Assignment Environment\n",
    "class TaskAssignmentEnv:\n",
    "    def __init__(self, tasks, employees, max_workload=20, alpha=0.5):\n",
    "        self.tasks = tasks\n",
    "        self.employees = employees\n",
    "        self.max_workload = max_workload\n",
    "        self.alpha = alpha\n",
    "        self.num_tasks = len(tasks)\n",
    "        self.num_employees = len(employees)\n",
    "\n",
    "        # State representation: a matrix (num_tasks x num_employees)\n",
    "        # indicating whether a particular employee is assigned to a task.\n",
    "        self.state = np.zeros((self.num_tasks, self.num_employees))\n",
    "        self.workloads = np.zeros(self.num_employees)\n",
    "\n",
    "    def valid_actions(self, task_idx):\n",
    "        \"\"\"\n",
    "        Return a list of employee indices that can be validly assigned to the given task.\n",
    "        Invalid if:\n",
    "        - The employee's workload would exceed max_workload\n",
    "        - The employee is a 'Dummy' employee (if any)\n",
    "        \"\"\"\n",
    "        valid = []\n",
    "        for i in range(self.num_employees):\n",
    "            if 'Dummy' in self.employees.iloc[i]['Role']:\n",
    "                # Skip dummy employees if they exist (padding)\n",
    "                continue\n",
    "            if self.workloads[i] + self.tasks.iloc[task_idx]['story_points'] <= self.max_workload:\n",
    "                valid.append(i)\n",
    "        return valid\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((self.num_tasks, self.num_employees))\n",
    "        self.workloads = np.zeros(self.num_employees)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, task_idx, employee_idx):\n",
    "        \"\"\"\n",
    "        Assign task_idx to employee_idx and calculate the reward.\n",
    "        \"\"\"\n",
    "        task = self.tasks.iloc[task_idx]\n",
    "        employee = self.employees.iloc[employee_idx]\n",
    "        task_skills = task[3:].values\n",
    "        employee_skills = employee[2:].values\n",
    "\n",
    "        # Similarity Score\n",
    "        similarity_score = calculate_weighted_euclidean_distance(employee_skills, task_skills, self.alpha)\n",
    "\n",
    "        # Update state & workload\n",
    "        self.state[task_idx, employee_idx] = 1\n",
    "        self.workloads[employee_idx] += task['story_points']\n",
    "\n",
    "        # Workload balance score: lower std means more balanced\n",
    "        std_workload = np.std(self.workloads)\n",
    "        workload_balance_score = 1 / (1 + std_workload)\n",
    "\n",
    "        # Combine into a single reward\n",
    "        reward = 0.8 * similarity_score + 0.2 * workload_balance_score\n",
    "\n",
    "        # Done if all tasks assigned or workload constraint violated\n",
    "        done = np.all(self.state.sum(axis=1)) or np.any(self.workloads > self.max_workload)\n",
    "\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmidLweEplG6"
   },
   "outputs": [],
   "source": [
    "# PPO Agent Class\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def get_action(self, state, valid_actions):\n",
    "        logits, _ = self.forward(state)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        if not valid_actions:\n",
    "            # If no valid actions, return 0 by default and a dummy log_prob\n",
    "            # Change from: return 0, torch.tensor(0.0)\n",
    "            return 0, torch.tensor([0.0]) # This ensures it's a 1-dimensional tensor\n",
    "        valid_probs = probs[valid_actions]\n",
    "        action_dist = Categorical(valid_probs)\n",
    "        action = action_dist.sample()\n",
    "        chosen_action = valid_actions[action.item()]\n",
    "        return chosen_action, action_dist.log_prob(action)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, agent, lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=4):\n",
    "        self.agent = agent\n",
    "        self.optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def compute_returns(self, rewards, dones, next_value):\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train(self, states, actions, log_probs, rewards, dones, next_value):\n",
    "        returns = self.compute_returns(rewards, dones, next_value)\n",
    "        returns = torch.tensor(returns).float()\n",
    "        states = torch.tensor(states).float()\n",
    "        actions = torch.tensor(actions).long()\n",
    "        log_probs = torch.tensor(log_probs).float()\n",
    "\n",
    "        total_loss, policy_loss, value_loss = 0.0, 0.0, 0.0\n",
    "        for _ in range(self.k_epochs):\n",
    "            logits, state_values = self.agent(states)\n",
    "            action_dist = Categorical(torch.softmax(logits, dim=-1))\n",
    "            new_log_probs = action_dist.log_prob(actions)\n",
    "\n",
    "            # Policy Loss\n",
    "            ratios = torch.exp(new_log_probs - log_probs)\n",
    "            advantages = returns - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Value Loss\n",
    "            value_loss = self.mse_loss(state_values.squeeze(), returns)\n",
    "\n",
    "            # Total Loss\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / self.k_epochs, policy_loss.item(), value_loss.item()\n",
    "\n",
    "    def save_transferable_weights(self, path):\n",
    "        transferable_state_dict = {\n",
    "            name: param for name, param in self.agent.state_dict().items() if \"actor\" not in name\n",
    "        }\n",
    "        torch.save(transferable_state_dict, path)\n",
    "\n",
    "    def load_transferable_weights(self, path):\n",
    "        \"\"\"\n",
    "        Load transferable weights from a checkpoint, ignoring mismatched dimensions.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to the checkpoint file.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        model_state = self.agent.state_dict()\n",
    "\n",
    "        # Update only matching keys\n",
    "        for name, param in checkpoint.items():\n",
    "            if name in model_state and model_state[name].size() == param.size():\n",
    "                model_state[name].copy_(param)\n",
    "            else:\n",
    "                print(f\"Skipping layer '{name}' due to size mismatch: {param.size()} vs {model_state[name].size()}\")\n",
    "\n",
    "        self.agent.load_state_dict(model_state, strict=False)\n",
    "        print(f\"Transferable weights loaded from {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utR4HNslpn_b",
    "outputId": "3026d723-84e0-4eb1-edc5-0da439b56d60"
   },
   "outputs": [],
   "source": [
    "# Training configurations\n",
    "num_iterations = 2  # We want to test with the first 2 iterations\n",
    "num_episodes_per_iteration = 500  # Adjust as needed\n",
    "convergence_threshold = 1e-2\n",
    "convergence_patience = 100\n",
    "\n",
    "# Lists to store overall metrics across curriculum\n",
    "all_similarity_scores = []\n",
    "all_workload_distributions = []\n",
    "all_std_workloads = []\n",
    "all_reward_history = []\n",
    "\n",
    "# Train iteratively\n",
    "agent = None\n",
    "ppo = None\n",
    "\n",
    "# Training loop with loss tracking by episode\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\n=== Curriculum Iteration {iteration} ===\")\n",
    "\n",
    "    # Load subset of data for this iteration\n",
    "    df_tasks, df_employees = load_curriculum_data(df_tasks_original, df_employees_original, iteration)\n",
    "\n",
    "    # Initialize environment for this iteration\n",
    "    env = TaskAssignmentEnv(df_tasks, df_employees)\n",
    "    state_dim = env.state.size\n",
    "    action_dim = env.num_employees\n",
    "\n",
    "    # Initialize or load the agent\n",
    "    if iteration == 1:\n",
    "        agent = PPOAgent(state_dim, action_dim)\n",
    "        ppo = PPO(agent)\n",
    "    else:\n",
    "        # Load transferable weights from the previous model\n",
    "        agent = PPOAgent(state_dim, action_dim)\n",
    "        ppo = PPO(agent)\n",
    "        ppo.load_transferable_weights(f\"/content/drive/MyDrive/Skripsi/Resources/Models/ppo_agent_iteration_{iteration-1}.pt\")\n",
    "\n",
    "    iteration_reward_history = []\n",
    "    iteration_similarity_scores = []\n",
    "    iteration_workload_distributions = []\n",
    "    iteration_std_workloads = []\n",
    "    task_assignments_strategy = {}\n",
    "\n",
    "    for episode in range(num_episodes_per_iteration):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        task_idx = 0\n",
    "        episode_similarity_scores = []\n",
    "        episode_workload = np.zeros(env.num_employees)\n",
    "        task_assignments_episode = {}\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state.flatten(), dtype=torch.float32)\n",
    "            valid_actions = env.valid_actions(task_idx=task_idx)\n",
    "            action, log_prob = agent.get_action(state_tensor, valid_actions)\n",
    "            next_state, reward, done = env.step(task_idx=task_idx, employee_idx=action)\n",
    "\n",
    "            # Track assignments\n",
    "            task = env.tasks.iloc[task_idx]\n",
    "            employee = env.employees.iloc[action]\n",
    "            task_assignments_episode[task['task_id']] = employee['employee_id']\n",
    "\n",
    "            # Skills indexing: tasks start at col 3, employees at col 2\n",
    "            t_skills = task[3:].values\n",
    "            e_skills = employee[2:].values\n",
    "\n",
    "            distance = calculate_weighted_euclidean_distance(e_skills, t_skills, env.alpha)\n",
    "            episode_similarity_scores.append(distance)\n",
    "            episode_workload[action] += task['story_points']\n",
    "\n",
    "            # Train PPO and log losses\n",
    "            loss, policy_loss, value_loss = ppo.train(\n",
    "                [state_tensor.numpy()], [action], [log_prob.item()], [reward], [done], next_value=0\n",
    "            )\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if np.any(env.state[task_idx]):\n",
    "                task_idx += 1\n",
    "            if task_idx >= env.num_tasks:\n",
    "                done = True\n",
    "\n",
    "        # Record metrics for this episode\n",
    "        iteration_similarity_scores.append(episode_similarity_scores)\n",
    "        iteration_workload_distributions.append(episode_workload)\n",
    "        iteration_std_workloads.append(np.std(episode_workload))\n",
    "        iteration_reward_history.append(episode_reward)\n",
    "\n",
    "        # Update final task assignment strategy\n",
    "        task_assignments_strategy = task_assignments_episode\n",
    "\n",
    "        # Log episode information\n",
    "        print(\n",
    "            f\"Episode {episode+1}, Reward: {episode_reward:.6f}, \"\n",
    "            f\"Loss: {loss:.6f}, Policy Loss: {policy_loss:.6f}, Value Loss: {value_loss:.6f}\"\n",
    "        )\n",
    "\n",
    "        # Check for convergence\n",
    "        if len(iteration_reward_history) > convergence_patience:\n",
    "            recent_rewards = iteration_reward_history[-convergence_patience:]\n",
    "            if max(recent_rewards) - min(recent_rewards) < convergence_threshold:\n",
    "                print(f\"Convergence achieved at episode {episode+1}. Stopping training early.\")\n",
    "                break\n",
    "\n",
    "    # Save transferable model weights\n",
    "    model_save_path = f\"/content/drive/MyDrive/Skripsi/Resources/Models/ppo_agent_iteration_{iteration}.pt\"\n",
    "    ppo.save_transferable_weights(model_save_path)\n",
    "    print(f\"Saved model for iteration {iteration} at {model_save_path}\")\n",
    "\n",
    "    # Log final task assignment strategy for this iteration\n",
    "    print(\"\\nFinal Task Assignment Strategy:\")\n",
    "    for task_id, emp_id in task_assignments_strategy.items():\n",
    "        print(f\"Task {task_id} assigned to Employee {emp_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_with_dimension_check(model, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load weights from a checkpoint into the model, only for layers with matching dimensions.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model instance to load weights into.\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model_state = model.state_dict()\n",
    "\n",
    "    for name, param in checkpoint.items():\n",
    "        if name in model_state and model_state[name].size() == param.size():\n",
    "            model_state[name].copy_(param)\n",
    "        else:\n",
    "            print(f\"Skipping layer '{name}' due to size mismatch: {param.size()} vs {model_state[name].size()}\")\n",
    "\n",
    "    model.load_state_dict(model_state, strict=False)\n",
    "    print(f\"Loaded weights with dimension check from {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qOP4Og3VsTZ_",
    "outputId": "b79cb813-7667-4c89-bfc9-d6ca885b09cf"
   },
   "outputs": [],
   "source": [
    "# After finishing all curriculum iterations, we now have a final trained model\n",
    "# Evaluate the final model on:\n",
    "# 1. iteration=1 dataset\n",
    "# 2. iteration=2 dataset\n",
    "# 3. new unseen dataset\n",
    "\n",
    "def evaluate_model(agent, tasks_df, employees_df):\n",
    "    \"\"\"\n",
    "    Evaluate the PPO agent on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        agent (PPOAgent): The agent to evaluate.\n",
    "        tasks_df (DataFrame): Task dataset.\n",
    "        employees_df (DataFrame): Employee dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Similarity scores and workload distribution.\n",
    "    \"\"\"\n",
    "    # Initialize environment for this dataset\n",
    "    env = TaskAssignmentEnv(tasks_df, employees_df)\n",
    "    state_dim = env.state.size\n",
    "    action_dim = env.num_employees\n",
    "\n",
    "    # Reinitialize agent with correct dimensions\n",
    "    agent = PPOAgent(state_dim, action_dim)  # Adjust dimensions dynamically\n",
    "    checkpoint_path = \"/content/drive/MyDrive/Skripsi/Resources/Models/ppo_agent_iteration_2.pt\"\n",
    "    load_state_with_dimension_check(agent, checkpoint_path)\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    task_idx = 0\n",
    "    similarity_scores_eval = []\n",
    "    workload_eval = np.zeros(env.num_employees)\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state.flatten(), dtype=torch.float32)\n",
    "        valid_actions = env.valid_actions(task_idx=task_idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, log_prob = agent.get_action(state_tensor, valid_actions)\n",
    "\n",
    "        next_state, reward, done = env.step(task_idx=task_idx, employee_idx=action)\n",
    "\n",
    "        # Track similarity and workload\n",
    "        task = env.tasks.iloc[task_idx]\n",
    "        employee = env.employees.iloc[action]\n",
    "        t_skills = task[3:].values\n",
    "        e_skills = employee[2:].values\n",
    "        distance = calculate_weighted_euclidean_distance(e_skills, t_skills, env.alpha)\n",
    "\n",
    "        similarity_scores_eval.append(distance)\n",
    "        workload_eval[action] += task['story_points']\n",
    "\n",
    "        state = next_state\n",
    "        if np.any(env.state[task_idx]):\n",
    "            task_idx += 1\n",
    "        if task_idx >= env.num_tasks:\n",
    "            done = True\n",
    "\n",
    "    return similarity_scores_eval, workload_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldv5Csqco7lB"
   },
   "outputs": [],
   "source": [
    "# Load final model (iteration=2)\n",
    "final_agent = PPOAgent(state_dim, action_dim)  # Reinitialize with current dimensions\n",
    "load_state_with_dimension_check(final_agent, \"/content/drive/MyDrive/Skripsi/Resources/Models/ppo_agent_iteration_2.pt\")\n",
    "\n",
    "# Evaluate on iteration=1 dataset\n",
    "iter1_tasks, iter1_emps = load_curriculum_data(df_tasks_original, df_employees_original, 1)\n",
    "iter1_similarity, iter1_workload = evaluate_model(final_agent, iter1_tasks, iter1_emps)\n",
    "\n",
    "# Evaluate on iteration=2 dataset\n",
    "iter2_tasks, iter2_emps = load_curriculum_data(df_tasks_original, df_employees_original, 2)\n",
    "iter2_similarity, iter2_workload = evaluate_model(final_agent, iter2_tasks, iter2_emps)\n",
    "\n",
    "# Evaluate on a new unseen dataset\n",
    "new_tasks = df_tasks_original.iloc[10:15].reset_index(drop=True)\n",
    "new_emps = df_employees_original.iloc[4:6].reset_index(drop=True)\n",
    "new_similarity, new_workload = evaluate_model(final_agent, new_tasks, new_emps)\n",
    "\n",
    "# Plot final evaluation similarity scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot([iter1_similarity, iter2_similarity, new_similarity], labels=[\"Iter1 Data\", \"Iter2 Data\", \"New Data\"])\n",
    "plt.title('Distribution of Similarity Scores on Different Evaluation Sets')\n",
    "plt.ylabel('Similarity Score')\n",
    "plt.xlabel('Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot workload distributions for evaluation\n",
    "def plot_workload_distribution(workloads, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(workloads)), workloads, tick_label=[f'E{i+1}' for i in range(len(workloads))])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Employee')\n",
    "    plt.ylabel('Total Workload')\n",
    "    plt.show()\n",
    "\n",
    "plot_workload_distribution(iter1_workload, 'Workload Distribution on Iteration 1 Data')\n",
    "plot_workload_distribution(iter2_workload, 'Workload Distribution on Iteration 2 Data')\n",
    "plot_workload_distribution(new_workload, 'Workload Distribution on New Unseen Data')\n",
    "\n",
    "# Plot std deviation progress during training (for demonstration, we plot only from iteration 2)\n",
    "# Flattening the lists for demonstration if you want a single plot\n",
    "all_std_flat = [val for iteration_list in all_std_workloads for val in iteration_list]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(all_std_flat, label='Std Dev of Workloads Across Curriculum')\n",
    "plt.title('Standard Deviation of Workloads Across Episodes in Curriculum')\n",
    "plt.xlabel('Episode (across all iterations)')\n",
    "plt.ylabel('Standard Deviation of Workloads')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
